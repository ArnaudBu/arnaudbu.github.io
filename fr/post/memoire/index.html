<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">


  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Approximation du bilan économique sous Solvabilité II via des méthodes d&#39;apprentissage automatique et application à l&#39;ORSA &middot; Statistiques, Actuariat et Philosophie</title>
  <meta name="description" content="" />

  
  <link type="text/css" rel="stylesheet" href="https://arnaudbu.github.com/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://arnaudbu.github.com/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://arnaudbu.github.com/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://arnaudbu.github.com/css/hyde.css">
  <link type="text/css" rel="stylesheet" href="https://arnaudbu.github.com/css/custom.css">
  <link type="text/css" rel="stylesheet" href="https://arnaudbu.github.com/lib/fontawesome-free-5.12.0-web/css/all.min.css">
  <link type="text/css" rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css">
  <link type="text/css" rel="stylesheet" href="https://arnaudbu.github.com/css/codefolding.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">




  
  

</head>

  <body class="theme-base-0c ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <img src="https://arnaudbu.github.com/img/avatar.png" alt="Avatar"  style="border-radius: 50%;width: 60%;  display: block; margin-left: auto; margin-right: auto;max-width:120px">
    <div class="sidebar-about">
      <a href="https://arnaudbu.github.com/"><h1>Statistiques, Actuariat et Philosophie</h1></a>
      <p class="lead">
       Recherche du sens de la vie grâce à la combinaison ultime de ces trois disciplines 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        
        <li><a href="https://arnaudbu.github.com/en">English version here</a></li>
        
        
      </ul>
    </nav>
    <p>
      <a href="https://github.com/arnaudbu/"> <span class='fab fa-github fa-2x'></span> </a><a href="https://ca.linkedin.com/in/arnaud-buzzi-03a42910b/en"> <span class='fab fa-linkedin fa-2x'></span> </a>
    </p>
    <p>© Arnaud Buzzi</p>
  </div>
</aside>
    <main class="content container">
    <div class="post">
  <h1>Approximation du bilan économique sous Solvabilité II via des méthodes d&#39;apprentissage automatique et application à l&#39;ORSA</h1>
  <time datetime=2017-05-12T00:00:00Z class="post-date">2017-05-12</time>
  



<div style="font-size: 200%;color: rgb(33, 123, 114);">M&eacute;moire d'actuariat</div>
<div class="intro">
				<p>Pour le premier article publi&eacute; sur ce site, il me paraissait opportun de choisir mon m&eacute;moire d'actuariat, qui est &agrave; ce jour le travail auquel j'ai consacr&eacute; le plus de temps. Celui-ci s'interroge sur l'utilisation de l'apprentissage automatique en assurance vie, dans le cadre de la manipulation de valeurs r&eacute;glementaires dont le calcul est compliqu&eacute;.</p>
				<p>Cette page est un r&eacute;sum&eacute; du m&eacute;moire, avec pour objectif de transmettre son principal message, &agrave; savoir qu'il faut conserver proprement <strong>toutes</strong> les donn&eacute;es des simulations effectu&eacute;, qui portent en elles une information tr&egrave;s riche.</p>
				<p>Lien vers le PDF du m&eacute;moire : <a href="/img/memoire/memoire.pdf">Approximation du bilan &eacute;conomique sous Solvabilit&eacute; II via des m&eacute;thodes d&apos;apprentissage automatique et application &agrave; l&apos;ORSA</a>
		</p>
</div>

<div id="orsa-et-apprentissage-automatique" class="section level2">
<h2>ORSA et Apprentissage automatique</h2>
<p>L’Own Risk Solvency Assesment (ORSA) est un outil interne d’analyse stratégique et d’aide à la décision, dont l’objectif est d’évaluer les risques spécifiques de l’entreprise qui le mène, de manière prospective et continue, sur un horizon de temps au moins égal au plan d’activité. Deux évaluations particulières, auxquelles s’intéresse ce mémoire, sont réalisées dans ce processus :</p>
<ol style="list-style-type: decimal">
<li>L’évaluation du besoin global de solvabilité de l’entreprise, appelé ORSA annuel dans l’étude. L’entreprise définit son profil propre de risque, liste les moyens à mettre en oeuvre pour y faire face et vérifie que ses risques sont maîtrisés pour différents scénarios.</li>
<li>L’évaluation du respect permanent des obligations réglementaires concernant la couverture du Solvency Capital Requirement (SCR), du Minimum Capital Requirement (MCR) et le respect des exigences concernant le calcul des provisions techniques, appelé ORSA permanent dans l’étude.</li>
</ol>
<p>Mener à bien ces deux études nécessite de réaliser de nombreux calculs relatifs au bilan d’une compagnie d’assurance, du fait de l’obligation d’évaluer plusieurs fois des grandeurs comme le Best Estimate of Liabilities (BEL) ou le SCR. Sachant que la durée du processus permettant d’obtenir ces valeurs se mesure généralement en jours, il est difficile de réaliser un ORSA de manière exhaustive et d’effectuer un grand nombre de tests. Une solution innovante à cette problématique est l’apprentissage automatique. À partir de données de simulations déjà réalisées, il est possible de calibrer un modèle capable de prédire la valeur du BEL, du SCR, ou du ratio de solvabilité d’une entreprise à partir des variables d’entrée du modèle Asset Liability Management (ALM). Ce procédé permet d’éliminer la contrainte de temps induite par les simulations requises, mais au prix d’une approximation, généralement d’autant plus grande que le nombre de données disponibles est faible. Ce mémoire s’interroge sur la possibilité d’utiliser des algorithmes d’apprentissage automatique pour estimer les trois grandeurs évoquées et d’appliquer les résultats aux deux points de l’ORSA mentionnés.</p>
</div>
<div id="modélisation-alm" class="section level2">
<h2>Modélisation ALM</h2>
<p>Afin de mettre en oeuvre la démarche suggérée, une modélisation ALM initiale est nécessaire. Étant donné que c’est en assurance vie que les calculs liés au bilan d’un assureur sont les plus complexes, à cause des relations qui existent entre actif et passif, l’étude s’intéressera au cas d’un fonds en euros. Bien que des hypothèses fortes et parfois trop simplificatrices d’un point de vue métier soient prises, la modélisation ALM développée présente un niveau de complexité suffisant pour la considérer pertinente par rapport aux standards actuels du marché. Le SCR est calculé à l’aide de la formule standard. Parmi les limitations les plus significatives se trouvent l’absence d’ajustement dans la formule standard et le rassemblement en une unique obligation de l’ensemble du portefeuille obligataire.</p>

<figure>
  <img src="/img/memoire/bilan_comptable.png" alt="Bilan Comptable"  >
    <center><figcaption>(a) Bilan comptable</figcaption></center>
</figure>
<figure>
  <img src="/img/memoire/bilan_eco_BOF.png" alt="Bilan &eacute;conomique" >
  <center><figcaption>(b) Bilan &eacute;conomique</figcaption></center>
</figure>
<center><figcaption style="clear:both; padding-top: 15px; margin-bottom: 20px">Figure 1 - Mod&eacute;lisation du bilan</figcaption></center>

<p>La figure 1 présente les bilans comptable et économique associés au fonds en euros étudié. Ces bilans sont simplifiés, et nombre d’éléments pourtant importants, comme la Provision pour Risque d’Exigibilité et la Risk Margin (RM), sont omis. La formule standard est quant à elle implémentée en agrégeant les sous-modules de risques présentés par la figure 2.</p>

<figure>
  <center><img src="/img/memoire/SCR.png" alt="Formule standard"  ></center>
    <center style="padding-top: 15px; margin-bottom: 20px"><figcaption>Figure 2 - Agr&eacute;gation des diff&eacute;rents sous-modules de SCR dans le cadre &eacute;tudi&eacute;</figcaption></center>
</figure>

<p>Les entrées du modèle ALM sont alors générées aléatoirement, avant que ne leur soit appliqué ledit modèle, pour constituer une base de données regroupant 11 201 observations du BEL, du SCR et du ratio de solvabilité du fonds étudié.</p>
</div>
<div id="apprentissage-automatique-supervisé-et-comparaison-de-modèles" class="section level2">
<h2>Apprentissage automatique supervisé et comparaison de modèles</h2>
<p>L’apprentissage automatique supervisé consiste à utiliser un ensemble d’observations, pour lesquelles les entrées et les sorties d’un modèle sous-jacent sont connues, afin de construire une fonction de prédiction des sorties au regard des entrées. La fonction de prédiction est calibrée en choisissant parmi les éléments d’un sous-espace fonctionnel caractérisé par le type d’algorithme mis en oeuvre. Plusieurs classes d’algorithmes sont alors testées :</p>
<ul>
<li>Les modèles linéaires gaussiens, qui supposent une fonction de prédiction linéaire par rapport à chacune des variables.</li>
<li>Les réseaux de neurones, inspirés du fonctionnement des neurones biologiques.</li>
<li>Les Séparateurs à Vaste Marge, qui reposent sur une séparation optimale des variables.</li>
<li>Les algorithmes CART, qui supposent que la fonction de prédiction est un arbre de décision binaire.</li>
<li>Les forêts aléatoires et le bagging, agrégations de plusieurs arbres CART indépendants.</li>
<li>Le tree boosting, qui est aussi une agrégation d’arbres, mais interdépendants, à travers sont implémentation la plus récente : XGBoost.</li>
</ul>
<p>Afin de comparer les capacités prédictives des modèles, la base de données est scindée en deux : 80% des observations serviront à calibrer les modèles, et les 20% restantes serviront à les évaluer. Les termes apprentissage et test seront respectivement employés pour les désigner.</p>

<figure>
  <img src="/img/memoire/modelCompBEPercent-1.png" alt="Comparaison sur BE"  >
    <center><figcaption>(a) Pour le BEL</figcaption></center>
</figure>
<figure>
  <img src="/img/memoire/modelCompSCRPercent-1.png" alt="Comparaison sur SCR" >
  <center><figcaption>(b) Pour le SCR</figcaption></center>
</figure>
<figure>
  <img src="/img/memoire/modelCompRatioPercent-1.png" alt="Comparaison sur Ratio" >
  <center><figcaption>(c) Pour le ratio de solvabilit&eacute;</figcaption></center>
</figure>
<center><figcaption style="clear:both; padding-top: 15px; margin-bottom: 20px">Figure 3 - Comparaison de mod&egrave;les</figcaption></center>

<p>La figure 3 compare les résultats de chaque algorithme en fournissant l’erreur en pourcentage sur chacune des observations de la base de test pour les trois grandeurs d’intérêt. Dans tous les cas, XGBoost est le modèle le plus performant.</p>
</div>
<div id="xgboost" class="section level2">
<h2>XGBoost</h2>
<p>Le tree boosting construit une fonction de prédiction sous la forme d’un ensemble de modèles prédictifs peu complexes, les arbres de décisions. Le modèle est construit de façon incrémentale, puisque chaque arbre tient compte des arbres précédemment introduits, dans le but de minimiser l’erreur globale. Le modèle fournit d’excellents résultats, dont les erreurs moyennes absolues et en pourcentage, sur les bases d’apprentissage et de test, sont données par le tableau ci-dessous.</p>

<div style="overflow-x:auto;">

<table>
<colgroup>
<col width="6%" />
<col width="16%" />
<col width="17%" />
<col width="16%" />
<col width="21%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Ordre de grandeur</th>
<th><span class="math inline">\(\epsilon^{train}\)</span></th>
<th><span class="math inline">\(\epsilon^{test}\)</span></th>
<th><span class="math inline">\(\epsilon^{train}_{\%}\)</span></th>
<th><span class="math inline">\(\epsilon^{test}_{\%}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BEL</td>
<td>98,91</td>
<td>0,09</td>
<td>0,52</td>
<td>0,09%</td>
<td>0,53%</td>
</tr>
<tr class="even">
<td>SCR</td>
<td>8,65</td>
<td>0,17</td>
<td>0,67</td>
<td>2,19%</td>
<td>8,31%</td>
</tr>
<tr class="odd">
<td>Ratio</td>
<td>2,60</td>
<td>0,06</td>
<td>0,19</td>
<td>1,96%</td>
<td>8,51%</td>
</tr>
</tbody>
</table>

</div>

<p>Les erreurs sur la base d’apprentissage sont très faibles, ce qui prouve que le modèle s’adapte très bien aux données. Les résultats obtenus sur la base de test montrent des erreurs absolues et relatives suffisamment basses pour attester des capacités prédictives du modèle dans le cas étudié. Ces bons résultats sont confirmés graphiquement par la figure 4, qui trace pour chacune des trois grandeurs d’intérêt les valeurs prédites par le modèle sur tous les éléments de la base de test en fonction des valeurs observées.</p>

<figure>
  <img src="/img/memoire/testAdeqResidusxgBE-1.png" alt="R&eacute;sidus sur BE"  >
    <center><figcaption>(a) Pour le BEL</figcaption></center>
</figure>
<figure>
  <img src="/img/memoire/testAdeqResidusxgSCR-1.png" alt="R&eacute;sidus sur SCR" >
  <center><figcaption>(b) Pour le SCR</figcaption></center>
</figure>
<figure>
  <img src="/img/memoire/testAdeqResidusxgRatio-1.png" alt="R&eacute;sidus sur Ratio" >
  <center><figcaption>(c) Pour le ratio de solvabilit&eacute;</figcaption></center>
</figure>
<center><figcaption style="clear:both; padding-top: 15px; margin-bottom: 20px">Figure 4 - R&eacute;sultats du mod&egrave;le</figcaption></center>

<p>Une autre grande qualité du modèle est qu’il ne nécessite pas énormément de données pour fournir de bons résultats, en atteste la figure 5, qui présente l’évolution de l’erreur moyenne commise sur la base de test en fonction de la proportion de données utilisée. Il apparaît qu’un quart des données suffit déjà à calibrer correctement le modèle XGBoost dans le contexte de l’étude.</p>

<center><figure>
  <center><img src="/img/memoire/nbDonneesCrossValAll-1.png" alt="Nombre de donn&eacute;es"  ></center>
    <center style="padding-top: 15px; margin-bottom: 20px"><figcaption>Figure 5 - R&eacute;sultats obtenus en fonction du nombre des donn&eacute;es</figcaption></center>
</figure></center>

</div>
<div id="possibilité-dutilisation-du-modèle-final" class="section level2">
<h2>Possibilité d’utilisation du modèle final</h2>
<p>Le modèle finalement calibré peut être utilisé afin de compléter l’ORSA annuel et l’ORSA permanent, en utilisant les possibilités qu’il offre pour générer instantanément plusieurs centaines de milliers de scénarios stochastiques.</p>

<figure>
  <img src="/img/memoire/ORSARatio-1.png" alt="Orsa ponctuel"  >
    <center><figcaption>(a) ORSA ponctuel</figcaption></center>
</figure>
<figure>
  <img src="/img/memoire/scenariosRatioclassifRatio-1.png" alt="Orsa permanent" >
  <center><figcaption>(b) ORSA permanent</figcaption></center>
</figure>
<center><figcaption style="clear:both; padding-top: 15px; margin-bottom: 20px">Figure 6 - Utilisations du mod&egrave;le</figcaption></center>

<p>La figure 6a, qui compare les estimations, pour le ratio, du modèle ALM et de la prédiction par apprentissage automatique pour trois scénarios différents sur cinq années de projection, montre qu’il n’est pas possible de suppléer purement et simplement le modèle ALM sur un faible nombre de points. Le modèle par apprentissage automatique a en effet du mal à coller à la courbe ALM. En revanche, il est possible, en passant par la génération d’un grand nombre de scénarios pour les entrées qui alimenteront le modèle calibré, de construire une analyse de sensibilités, comme sur la figure 6b. Cette dernière montre la variation de chaque variable d’entrée après 1 an par rapport à sa valeur initiale en fonction du ratio de solvabilité, et pousse ainsi l’analyse dans des zones inaccessibles auparavant. Cette figure permet par exemple de se rendre compte que les taux de chargement appliqués ou les taux de frais de prestation n’influent pas outre mesure sur la valeur du ratio de solvabilité, si celui-ci se situe aux environs de 2,4.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Les méthodes d’apprentissage automatique apportent un complément intéressant à l’ORSA, et ce pour un nombre de données relativement faible. De cette étude a ainsi émergé la conviction que les assureurs devraient investir dans les technologies qui leur permettraient de faire tourner en continu leurs modèles ALM pour obtenir une base de données permettant l’application de la méthodologie présentée, ou a minima mettre en place un processus de conservation des résultats des simulations ALM menées, afin d’exploiter par la suite ces précieuses informations.</p>
</div>

</div>


    </main>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-99074891-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  </body>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    
    

<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>





  
  
  
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  
  
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script> 
  
</html>
